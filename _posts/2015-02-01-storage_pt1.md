---
layout:     post
title:      Storage (part one)
date:       2015-02-01 21:00:00
summary: Time series data can be difficult to store. It is often generated during a long time periods and can be huge. This can gives us two first requirements...
categories: akumuli
---

Let's talk about storage organization in [akumuli](https://github.com/akumuli/Akumuli).
Time series data can be difficult to store. It is often generated during a long time periods and can be huge. This can gives us two first requirements: compression and crash recovery. Significance of the compression is obvious (let's talk about it later). Crash recovery is a must if we don't want to loose all data when somebody have killed the app using `kill -9` command or because of power outage. Databases usually implemented in such way that prevents major data loss in such situations. They write new data to the write ahead log (WAL) first, fsync and only after that data can be written to the main storage.

Akumuli uses different approach. All data stored in one large WAL that consists from many volumes. Each volume allocated beforehand and should have a fixed size (4Gb). WAL is append only. One can append new values or erase volume entirely (this happens regullary to clean up some space for the new data). What makes akumuli special is that there is no main data store. WAL is itself a main data store that can be queried!

#### Volume structure
Each volume starts from header. This header contains different useful information like volume size, number of data chunks stored, min and max timestamp inside volume and so on. One of the most important pieces of information in the header is an offset of the most recent data chunk.

Header is followed by the indirection vector. Indirection vector is an array of the offsets of the data chunks. This array is variably sized. When new chunk written to the volume its offset should be appended to the indirection vector. Offsets of every chunk is stored in the indirection vector for quick access. Variably sized data chunks could be quickly fetched in random order using volume's indirection vector.

Data chunk contains compressed time-series data from some time-range. Time-ranges of the different data chunks didn't overlap. Data chunks appended to the volume starting from the end. Volume is full when there is no space between indirection vector and the last data chunk.

![Volume diagram](/images/volume.png)

This scheme is similar to B-tree node structure in real databases except volume is huge compared to B-tree node. This scheme allows akumuli to write data really fast. Tests shows that data can be written to the akumuli with really high pace - more then 1M writes per second on the m3.xlarge instance (with max crash recovery level and something about 6M with relaxed crash recovery).

#### Crash recovery
Akumuli msyncs data after each write in two step process. First msync triggered after new chunk was appended to the volume and its offset was stored in the indirection vector. Second msync triggered after update of the crash recovery register. This register stores index of the newest data chunk. During start-up akumuli gets last chunk index from this crash recovery register. If failure occured during the first or second flush (or in between) this register would contain index of the previous chunk. If everything is OK it would contain index of the most recent data chunk.

This may sound like a very slow process (lots of msync calls) but this isn't a case because all writes is sequential and msync operation can be performed by the OS really fast.

#### Write trick
Akumuli maps all volumes into its address space. This can cause slowdown during write if volume already contains data. Operating system doesn't know that we trying to rewrite mapped file. Because of that it reads data into memory first. This can cause huge slowdown. To workaround this akumuli uses one neat trick. When volume needs to be overwritten we need to truncate file to zero size and then expand it back. After that operating system will know that file is empty and nothing need to be read back.

In the next part of this article I will talk about data compression. Stay tuned.
